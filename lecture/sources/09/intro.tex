

% \begin{frame}
%   Multi Layer Perceptron
%   \note{
%     \begin{itemize}
%       \item As in the extension of Linear Regression for Polynomials (previous lecture) we map the input in a non-linear way into another space, in which we try to separate the data linearly.
%       \item However, with the multi layer neural network we learn these mappings.
%     \end{itemize}
%   }
% \end{frame}


% \begin{frame}
%   Multi Layer Perceptron
%   \note{
%     \begin{itemize}
%       \item Non-linear activation functions.
%     \end{itemize}
%   }
% \end{frame}


% \begin{frame}
%   Multi Layer Perceptron
%   \note{
%     \begin{itemize}
%       \item Comparison to Classical Computer Vision.
%     \end{itemize}
%   }
% \end{frame}




% \begin{frame}
%   Backpropagation
%   \note{
%     \begin{itemize}
%       \item Gradient computation is expensive
%       \item
%       \item
%       \item
%     \end{itemize}
%   }
% \end{frame}


% \begin{frame}
%   ``With great power comes great overfitting`` (Joseph Redmon)
%   \note{
%     \begin{itemize}
%       \item Millions of parameters!
%       \item
%     \end{itemize}
%   }
% \end{frame}



% \begin{frame}
%   Avoid Overfitting
%   \note{
%     \begin{itemize}
%       \item Early stopping
%       \item
%     \end{itemize}
%   }
% \end{frame}


% \begin{frame}
%   Avoid Overfitting
%   \note{
%     \begin{itemize}
%       \item Regularization
%       \item
%     \end{itemize}
%   }
% \end{frame}
